{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ab57a8",
   "metadata": {},
   "source": [
    "# Q&A pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e18f33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: sentence-transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.13.4)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (1.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (4.26.1)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (0.12.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (1.8.1)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.1.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U datasets pandas sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb58ef3",
   "metadata": {},
   "source": [
    "### Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aea4470",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreeaioanatudor/Desktop/Uni/Msc/NLP/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset ik-nlp-22_slp (/Users/andreeaioanatudor/.cache/huggingface/datasets/GroNLP___ik-nlp-22_slp/paragraphs/1.0.0/6c89281b2028a8a126102dda2c3fb94b1a5ccea59943d26857ae138c7aa782f8)\n",
      "100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 136.07it/s]\n",
      "Found cached dataset ik-nlp-22_slp (/Users/andreeaioanatudor/.cache/huggingface/datasets/GroNLP___ik-nlp-22_slp/questions/1.0.0/6c89281b2028a8a126102dda2c3fb94b1a5ccea59943d26857ae138c7aa782f8)\n",
      "100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 889.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['n_chapter', 'chapter', 'n_section', 'section', 'n_subsection', 'subsection', 'text'],\n",
       "        num_rows: 1697\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "paragraphs_dataset = load_dataset(\"GroNLP/ik-nlp-22_slp\", 'paragraphs')\n",
    "questions_dataset = load_dataset('GroNLP/ik-nlp-22_slp', 'questions')\n",
    "\n",
    "paragraphs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e8bd57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "paragraphs = paragraphs_dataset['train'].to_pandas()\n",
    "questions = questions_dataset['test'].to_pandas()\n",
    "\n",
    "# only paragraphs text and questions \n",
    "paragraphs_list = list(paragraphs['text'])\n",
    "questions_list = list(questions['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d860b",
   "metadata": {},
   "source": [
    "### Set the parameters for Semantic Similarity\n",
    "`model_name` - pre-trained model used for emebdding\n",
    "\n",
    "`k` - the top k paragraphs retrieved for each question (the top k highest scores)\n",
    "\n",
    "`unique_chapter` - if set to `True`, only the paragraphs coming from the same chapter will be used in the QA system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "118d9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# model_name = \"sentence-transformers/masmarco-distilbert-base-tas-b\"\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "k = 10\n",
    "# k = 20\n",
    "\n",
    "unique_chapter = True\n",
    "# unique_chapter = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada5f23",
   "metadata": {},
   "source": [
    "### Information retrieval using Semantic Similarity\n",
    "Embed the paragraphs and questions. Then retrieve the top 10 paragraphs with the highest scores for each question (semantic similarity using cosine similarity) and store their indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d59e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# paragraphs embeddings\n",
    "embeddings = model.encode(paragraphs_list, convert_to_tensor=True)\n",
    "\n",
    "# questions embeddings\n",
    "q_embeddings = model.encode(questions_list, convert_to_tensor=True)\n",
    "\n",
    "# top k semantic similarity score paragraphs for each question (using cosine similarity)\n",
    "sem_search_scores = util.semantic_search(query_embeddings=q_embeddings, corpus_embeddings=embeddings, top_k=k)\n",
    "\n",
    "# stop the timer for information retrieval\n",
    "retrieval_end_time = time.time()\n",
    "\n",
    "# elapsed time for embedding + semantic search\n",
    "semantic_search_elapsed_time = retrieval_end_time - start_time\n",
    "\n",
    "contexts_idx = []\n",
    "contexts_pos = []\n",
    "\n",
    "# store the indexes of the retrieved paragraphs\n",
    "# and the start pos of each paragraph from a context \n",
    "for scores in sem_search_scores:\n",
    "    answers_idx = []\n",
    "    answers_pos = []\n",
    "    start_pos = 0\n",
    "    for answer in scores:\n",
    "        idx = answer['corpus_id']\n",
    "        answers_idx.append(idx)\n",
    "        answers_pos.append(start_pos)\n",
    "        start_pos += len(paragraphs_list[idx])\n",
    "    contexts_idx.append(answers_idx)\n",
    "    contexts_pos.append(answers_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d63f7",
   "metadata": {},
   "source": [
    "### Information Retrieval using an Instructable Model\n",
    "For each question, ask an instructable model which paragraphs are relevant to answer the question. Then, store all the relevant paragraphs (their indexes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baceddf6",
   "metadata": {},
   "source": [
    "Tried to use multithreading, but it still takes too long - can't run it for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d470763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import threading\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# import torch\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# # Define the function that each thread will execute \n",
    "# def get_relevant_context(question):\n",
    "#     answers_idx = []\n",
    "#     # Load the Flan-T5-Base model and tokenizer\n",
    "#     model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "#     tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    \n",
    "#     # find the relevant paragraphs for the question\n",
    "# #     for idx, context in enumerate(paragraphs_list):\n",
    "#     for idx, context in enumerate(paragraphs_list[0:10]):\n",
    "#         # Encode the input as a T5 sequence\n",
    "#         input_str = \"question: '{}' context: '{}' Is this context relevant for answering the question? Answer yes or no.\".format(question, context)\n",
    "#         input_ids = tokenizer.encode(input_str, return_tensors=\"pt\")\n",
    "\n",
    "#         # Generate an answer using the model\n",
    "#         outputs = model.generate(input_ids=input_ids)\n",
    "#         answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#         # if a paragraph is relevant for the question, store its index\n",
    "#         if 'yes' in answer.lower():\n",
    "#             answers_idx.append(idx)\n",
    "            \n",
    "#     return answers_idx\n",
    "\n",
    "# # Define the function that each thread will execute\n",
    "# def worker(questions, contexts_idx, start, end):\n",
    "#     for i in range(start, end):\n",
    "#         contexts_idx[i] = get_relevant_context(questions[i])\n",
    "\n",
    "# # Create a list to store the results (the indexes of the relevant paragraphs)\n",
    "# contexts_idx1 = [None] * len(questions_list)\n",
    "\n",
    "# num_threads = 15\n",
    "# chunk_size = len(questions_list) // num_threads\n",
    "\n",
    "# threads = []\n",
    "\n",
    "# # Create and start each thread\n",
    "# for i in range(num_threads):\n",
    "#     start = i * chunk_size\n",
    "#     end = start + chunk_size\n",
    "#     if i == num_threads - 1:\n",
    "#         end = len(questions_list)\n",
    "#     t = threading.Thread(target=worker, args=(questions_list, contexts_idx1, start, end))\n",
    "#     t.start()\n",
    "#     threads.append(t)\n",
    "\n",
    "# # Create a tqdm progress bar\n",
    "# with tqdm(total=len(questions_list)) as pbar:\n",
    "#     # Wait for all threads to finish\n",
    "#     for t in threads:\n",
    "#         t.join()\n",
    "#         # Update the progress bar\n",
    "#         pbar.update(chunk_size)\n",
    "\n",
    "# # The results list now contains the incremented numbers\n",
    "# print(contexts_idx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6509822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# # Load the Flan-T5-Base model and tokenizer\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# contexts_idx2 = []\n",
    "\n",
    "# for question in questions_list:\n",
    "#     answers_idx = []\n",
    "#     for idx, context in enumerate(paragraphs_list):\n",
    "#         # Encode the input as a T5 sequence\n",
    "#         input_str = \"question: '{}' context: '{}' Is this context relevant for answering the question? Answer yes or no.\".format(question, context)\n",
    "#         input_ids = tokenizer.encode(input_str, return_tensors=\"pt\")\n",
    "\n",
    "#         # Generate an answer using the model\n",
    "#         outputs = model.generate(input_ids=input_ids, max_length=32, num_beams=4)\n",
    "#         answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "#         # if a paragraph is relevant for the question, store its index\n",
    "#         if 'yes' in answer.lower():\n",
    "#             answers_idx.append(idx)\n",
    "        \n",
    "#     contexts_idx2.append(answers_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b402a",
   "metadata": {},
   "source": [
    "### Define the context for each question\n",
    "From the retrieved paragraphs of each question, keep the ones coming from the same, most common chapter, and create the context to use in the QA system. If all the top 10 (most relevant) paragraphs come from different chapters, only the first one will be used.\n",
    "\n",
    "For many sections/subsections, the value is 'nan'. So I only used chapter.\n",
    "\n",
    "`unique_chapter` must be set to `True`, otherwise all retrieved k paragraphs will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3300559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if unique_chapter:\n",
    "    book_part = 'chapter'\n",
    "    # book_part = 'n_section'\n",
    "    # book_part = 'n_subsection'\n",
    "    contexts = []\n",
    "    contexts_chapters = []\n",
    "\n",
    "    for answers_idx in contexts_idx:\n",
    "        # find the most common chapter of the k retrieved paragraphs\n",
    "        chapters = []\n",
    "        for idx in answers_idx:\n",
    "            chapters.append(paragraphs[book_part][idx])\n",
    "\n",
    "        most_common_chapter = max(chapters, key=chapters.count)\n",
    "        contexts_chapters.append(most_common_chapter)\n",
    "\n",
    "        # store the retrieved paragraphs coming from the most common chapter (and their indexes)\n",
    "        context = []\n",
    "        context_chap = []\n",
    "        for idx in answers_idx:\n",
    "            if paragraphs[book_part][idx] == most_common_chapter:\n",
    "                context.append(paragraphs['text'][idx])\n",
    "\n",
    "        contexts.append('. '.join(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306d1225",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not unique_chapter:\n",
    "    contexts = []\n",
    "    all_contexts_chapters = []\n",
    "    \n",
    "    for answers_idx in contexts_idx:\n",
    "        # append the chapter of each paragraph and\n",
    "        # concatenate all paragraphs to create the context\n",
    "        chapters = []\n",
    "        context = []\n",
    "        \n",
    "        for idx in answers_idx:\n",
    "            chapters.append(paragraphs[book_part][idx])\n",
    "            context.append(paragraphs['text'][idx])\n",
    "            \n",
    "        all_contexts_chapters.append(chapters)\n",
    "        contexts.append('. '.join(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfab89",
   "metadata": {},
   "source": [
    "### QA system\n",
    "Use the Inference API to access the roberta-base-squad2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a59900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def query(payload):\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88ae02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'deepset/roberta-base-squad2'\n",
    "API_TOKEN = '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'\n",
    "API_URL = \"https://api-inference.huggingface.co/models/\" + model\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "answers = []\n",
    "\n",
    "for idx, question in enumerate(questions_list):\n",
    "    answers.append(query(\n",
    "        {\n",
    "            \"inputs\": {\n",
    "                \"question\": question,\n",
    "                \"context\": contexts[idx]\n",
    "            }\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# total elapsed time (embedding + semantic search + defining context + question answering)\n",
    "total_elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b005b27",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Print the answers of the system and the actual answers.\n",
    "Then, we compute the cosine similarity for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e371d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_answer_location(answer, q_index):\n",
    "    start_pos = answer['start']\n",
    "    end_pos = answer['end']\n",
    "    idx = None\n",
    "    \n",
    "    for i in range(len(context)-1):\n",
    "        if start_pos > contexts_pos[q_index][i] and end_pos < contexts_pos[q_index][i+1]:\n",
    "            idx = contexts_idx[q_index][i]\n",
    "            \n",
    "    if idx is None:\n",
    "        idx = contexts_idx[q_index][-1]\n",
    "    \n",
    "    return {\n",
    "        'chapter': paragraphs['chapter'][idx],\n",
    "        'section': paragraphs['section'][idx],\n",
    "        'subsection': paragraphs['subsection'][idx]\n",
    "    }\n",
    "\n",
    "\n",
    "def get_location_str(location):\n",
    "    loc_str = ''\n",
    "    loc_keys = ['chapter', 'section', 'subsection']\n",
    "    \n",
    "    for key in loc_keys:\n",
    "        loc_str += f\"{key} {location[key]}, \"\n",
    "        \n",
    "    return loc_str[:-2]\n",
    "\n",
    "\n",
    "def compare_book_location(ans_loc, q_index):\n",
    "    for key in ans_loc.keys():\n",
    "        if questions[key][q_index] != ans_loc[key]:\n",
    "            return f\"different {key}\"\n",
    "    return \"same location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae021431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter Regular Expressions, section Regular Expressions, subsection Basic Regular Expressions\n"
     ]
    }
   ],
   "source": [
    "print(get_location_str(questions.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e52c44d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of the Kleene star in Regex?\n",
      "system answer:\t zero or more occurrences of the immediately previous character or regular expression\".\n",
      "real answer:\t The Kleene star means \"zero or more occurrences of the immediately previous character or regular expression\"\n",
      "\n",
      "What is the usage of the Regex lookahead operator \"?=\"?\n",
      "system answer:\t disjunction operator\n",
      "real answer:\t The operator (?= pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn’t advance.\n",
      "predicted answer: from  chapter Regular Expressions, section Regular Expressions, subsection Disjunction, Grouping and Precendence\n",
      "actual answer:    from  chapter Regular Expressions, section Regular Expressions, subsection Lookahead Assertions\n",
      "\n",
      "What are the most common steps in a text normalization process?\n",
      "system answer:\t frequency computation\n",
      "real answer:\t 1. Tokenizing (segmenting) words 2. Normalizing word formats 3. Segmenting sentences\n",
      "predicted answer: from  chapter Machine Translation and Encoder-Decoder Models, section Some practical details on building MT systems, subsection MT corpora\n",
      "actual answer:    from  chapter Regular Expressions, section Text Normalization, subsection nan\n",
      "\n",
      "What are the two most common components of a tokenization scheme?\n",
      "system answer:\t a token learner, and a token segmenter\n",
      "real answer:\t a token learner, and a token segmenter\n",
      "\n",
      "What is the purpose of a token segmenter?\n",
      "system answer:\t takes a raw test sentence and segments it into the tokens in the vocabulary\n",
      "real answer:\t The token segmenter takes a raw test sentence and segments it into the tokens in the vocabulary.\n",
      "\n",
      "What is the purpose of a token learner in the BPE algorithm?\n",
      "system answer:\t learning a vocabulary by iteratively merging tokens\n",
      "real answer:\t taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens.\n",
      "\n",
      "What is word normalization?\n",
      "system answer:\t the task of putting words/tokens in a standard format\n",
      "real answer:\t Word normalization is the task of putting words/tokens in a standard format\n",
      "\n",
      "How is lemmatization performed?\n",
      "system answer:\t complete morphological parsing of the word\n",
      "real answer:\t The most sophisticated methods for lemmatization involve complete morphological parsing of the word.\n",
      "\n",
      "What is lemmatization?\n",
      "system answer:\t the task of determining that two words have the same root\n",
      "real answer:\t Lemmatization is the task of determining that two words have the same root, despite their surface differences.\n",
      "predicted answer: from  chapter Regular Expressions, section nan, subsection nan\n",
      "actual answer:    from  chapter Regular Expressions, section Text Normalization, subsection Word Normalization, Lemmatization and Stemming\n",
      "\n",
      "How is the minimum edit distance between two strings defined?\n",
      "system answer:\t an alignment\n",
      "real answer:\t the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.\n",
      "predicted answer: from  chapter Regular Expressions, section nan, subsection nan\n",
      "actual answer:    from  chapter Regular Expressions, section Minimum Edit Distance, subsection nan\n",
      "\n",
      "What is a language model?\n",
      "system answer:\t Probabilistic\n",
      "real answer:\t Models that assign probabilities to sequences of words are called language models\n",
      "predicted answer: from  chapter Neural Networks and Neural Language Models, section Training the neural language model, subsection nan\n",
      "actual answer:    from  chapter N-gram Language Models, section nan, subsection nan\n",
      "\n",
      "How can we estimate the probability of a word?\n",
      "system answer:\t by multiplying together a number of conditional probabilities\n",
      "real answer:\t from relative frequency counts\n",
      "\n",
      "What is a Markov model?\n",
      "system answer:\t first-order hidden\n",
      "real answer:\t Markov models are the class of probabilistic models Markov that assume we can predict the probability of some future unit without looking too far into the past.\n",
      "predicted answer: from  chapter Sequence Labeling for Parts of Speech and Named Entities, section HMM Part-of-Speech Tagging, subsection The Hidden Markov Model\n",
      "actual answer:    from  chapter N-gram Language Models, section N-Grams, subsection nan\n",
      "\n",
      "What technique can be used to estimate n-gram probabilities?\n",
      "system answer:\t Smoothing algorithms\n",
      "real answer:\t maximum likelihood estimation or MLE\n",
      "predicted answer: from  chapter N-gram Language Models, section Summary, subsection nan\n",
      "actual answer:    from  chapter N-gram Language Models, section N-Grams, subsection nan\n",
      "\n",
      "What is the difference between bigram and trigram models?\n",
      "system answer:\t condition on the previous two words rather than the previous word\n",
      "real answer:\t condition on the previous two words rather than the previous word\n",
      "\n",
      "How can two probabilisting language models be compared on a test set?\n",
      "system answer:\t the better model is the one that has a tighter fit to the test data\n",
      "real answer:\t whichever model assigns a higher probability to the test set-meaning it more accurately predicts the test set-is a better model\n",
      "predicted answer: from  chapter N-gram Language Models, section Generalization and Zeros, subsection nan\n",
      "actual answer:    from  chapter N-gram Language Models, section Evaluating Language Models, subsection nan\n",
      "\n",
      "What is the perplexity of a language model?\n",
      "system answer:\t the inverse probability of the test set\n",
      "real answer:\t perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words.\n",
      "\n",
      "How can language models be prevented from assigning zero probability to unknown words?\n",
      "system answer:\t give it to the events we've never seen\n",
      "real answer:\t smoothing or discounting\n",
      "predicted answer: from  chapter N-gram Language Models, section Generalization and Zeros, subsection nan\n",
      "actual answer:    from  chapter N-gram Language Models, section Smoothing, subsection nan\n",
      "\n",
      "What technique can be used to shrink an n-gram language model?\n",
      "system answer:\t pruning\n",
      "real answer:\t pruning\n",
      "\n",
      "When is a stochastic process said to be stationary?\n",
      "system answer:\t yesterday, Monday\n",
      "real answer:\t A stochastic process is said to be stationary if the probabilities it assigns to a Stationary sequence are invariant with respect to shifts in the time index.\n",
      "\n",
      "Why are simplifying assumptions needed when using a Naive Bayes Classifier?\n",
      "system answer:\t huge numbers of parameters and impossibly large training sets\n",
      "real answer:\t without some simplifying assumptions, estimating the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets.\n",
      "\n",
      "What can be done to optimize Naive Bayes when an insufficient amount of labeled data is present?\n",
      "system answer:\t add-δ smoothing and information-based feature selection\n",
      "real answer:\t derive the positive and negative word features from sentiment lexicons, lists of words that are pre-sentiment lexicons annotated with positive or negative sentiment.\n",
      "predicted answer: from  chapter Naive Bayes and Sentiment Classification, section Training the Naive Bayes Classifier, subsection nan\n",
      "actual answer:    from  chapter Naive Bayes and Sentiment Classification, section Optimizing for Sentiment Analysis, subsection nan\n",
      "\n",
      "What type of features can be used to train a Naive Bayes classifier?\n",
      "system answer:\t probabilistic\n",
      "real answer:\t dictionaries, URLs, email addresses, network features, phrases\n",
      "predicted answer: from  chapter Naive Bayes and Sentiment Classification, section nan, subsection nan\n",
      "actual answer:    from  chapter Naive Bayes and Sentiment Classification, section Naive Bayes as a Language Model, subsection nan\n",
      "\n",
      "Why is accuracy rarely used alone for unbalanced text classification tasks?\n",
      "system answer:\t accuracy doesn't work well when the classes are unbalanced\n",
      "real answer:\t because accuracy doesn't work well when the classes are unbalanced\n",
      "\n",
      "What are folds in cross-validation?\n",
      "system answer:\t k disjoint subsets\n",
      "real answer:\t k disjoints subsets\n",
      "\n",
      "What is the purpose of logistic regression?\n",
      "system answer:\t one of the most important analytic tools in the social and natural sciences\n",
      "real answer:\t The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation.\n",
      "predicted answer: from  chapter Logistic Regression, section nan, subsection nan\n",
      "actual answer:    from  chapter Logistic Regression, section Classification: the Sigmoid, subsection nan\n",
      "\n",
      "What is gradient descent?\n",
      "system answer:\t a method that finds a minimum of a function\n",
      "real answer:\t Gradient descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters θ ) the function's slope is rising the most steeply, and moving in the opposite direction.\n",
      "\n",
      "What is stochastic gradient descent?\n",
      "system answer:\t an online algorithm\n",
      "real answer:\t Stochastic gradient descent is an online algorithm that minimizes the loss function by computing its gradient after each training example\n",
      "predicted answer: from  chapter Logistic Regression, section Gradient Descent, subsection The Stochastic Gradient Descent Algorithm\n",
      "actual answer:    from  chapter Logistic Regression, section Gradient Descent, subsection nan\n",
      "\n",
      "Which regularization technique is easier to optimize?\n",
      "system answer:\t L2\n",
      "real answer:\t L2 regularization is easier to optimize because of its simple derivative\n",
      "\n",
      "What is the other name of multinomial logistic regression?\n",
      "system answer:\t softmax regression\n",
      "real answer:\t softmax regression\n",
      "\n",
      "What are word connotations?\n",
      "system answer:\t affective meanings\n",
      "real answer:\t the aspects of a word's meaning that are related to a writer or reader's emotions, sentiment, opinions, or evaluations\n",
      "\n",
      "What is the idea behind vector semantics?\n",
      "system answer:\t word meaning could be represented as a point in space\n",
      "real answer:\t The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of word neighbors.\n",
      "predicted answer: from  chapter Vector Semantics and Embeddings, section Lexical Semantics, subsection nan\n",
      "actual answer:    from  chapter Vector Semantics and Embeddings, section Vector Semantics, subsection nan\n",
      "\n",
      "What do rows represent in a term-document matrix?\n",
      "system answer:\t each word (term) in the vocabulary\n",
      "real answer:\t a word in the vocabulary\n",
      "predicted answer: from  chapter Vector Semantics and Embeddings, section Summary, subsection nan\n",
      "actual answer:    from  chapter Vector Semantics and Embeddings, section Words and Vectors, subsection nan\n",
      "\n",
      "What do columns represent in a term-document matrix?\n",
      "system answer:\t a document from some collection of documents\n",
      "real answer:\t a document from some collection of documents\n",
      "predicted answer: from  chapter Vector Semantics and Embeddings, section Words and Vectors, subsection Vectors and documents\n",
      "actual answer:    from  chapter Vector Semantics and Embeddings, section Words and Vectors, subsection nan\n",
      "\n",
      "Why can dot product be used as a similarity metric?\n",
      "system answer:\t it will tend to be high\n",
      "real answer:\t The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions.\n",
      "\n",
      "What is the intuition behind PPMI?\n",
      "system answer:\t the best way to weigh the association between two words\n",
      "real answer:\t PPMI draws on the intuition that the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.\n",
      "\n",
      "What is a centroid?\n",
      "system answer:\t the multidimensional version of the mean\n",
      "real answer:\t a multidimensional version of the mean\n",
      "\n",
      "What are the three type of nodes in a feedforward neural network?\n",
      "system answer:\t input units, hidden units, and output units\n",
      "real answer:\t input units, hidden units, and output units\n",
      "\n",
      "What is the output of the output layer's softmax in a neural language model?\n",
      "system answer:\t outputsŷ t ,\n",
      "real answer:\t a probability distribution over words\n",
      "predicted answer: from  chapter Deep Learning Architectures for Sequence Processing, section RNNs as Language Models, subsection nan\n",
      "actual answer:    from  chapter Neural Networks and Neural Language Models, section Feedforward Neural Language Modeling, subsection Forward inference in the neural language model\n",
      "\n",
      "What is a computation graph?\n",
      "system answer:\t a representation of the process of computing a mathematical expression\n",
      "real answer:\t A computation graph is a representation of the process of computing a mathematical expression, in which the computation is broken down into separate operations, each of which is modeled as a node in a graph.\n",
      "\n",
      "What is part-of-speech tagging?\n",
      "system answer:\t there is no segmentation problem\n",
      "real answer:\t Part-of-speech tagging is the process of assigning a part-of-speech to each word in a text.\n",
      "predicted answer: from  chapter Naive Bayes and Sentiment Classification, section nan, subsection nan\n",
      "actual answer:    from  chapter Sequence Labeling for Parts of Speech and Named Entities, section Part-of-Speech Tagging, subsection nan\n",
      "\n",
      "What are named entities?\n",
      "system answer:\t useful clues to POS sentence structure and meaning\n",
      "real answer:\t A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization.\n",
      "predicted answer: from  chapter Sequence Labeling for Parts of Speech and Named Entities, section nan, subsection nan\n",
      "actual answer:    from  chapter Sequence Labeling for Parts of Speech and Named Entities, section Named Entities and Named Entity Tagging, subsection nan\n",
      "\n",
      "What is a Hidden Markov Model?\n",
      "system answer:\t sequence labeling algorithm\n",
      "real answer:\t An HMM is a probabilistic sequence model\n",
      "predicted answer: from  chapter Sequence Labeling for Parts of Speech and Named Entities, section HMM Part-of-Speech Tagging, subsection The components of a HMM tagger\n",
      "actual answer:    from  chapter Sequence Labeling for Parts of Speech and Named Entities, section HMM Part-of-Speech Tagging, subsection nan\n",
      "\n",
      "Why is limited context a limitation of feedforward neural networks?\n",
      "system answer:\t Anything outside the context window has no impact on the decision being made\n",
      "real answer:\t Anything outside the context window has no impact on the decision being made.\n",
      "\n",
      "What is a recurrent neural network?\n",
      "system answer:\t any network that contains a cycle within its network connections\n",
      "real answer:\t any network that contains a cycle within its network connections\n",
      "\n",
      "What is an autoregressive language model?\n",
      "system answer:\t predicts a value at time t based on a linear function of the previous values\n",
      "real answer:\t a model that predicts a value at time t based on a linear function of the previous values at times t − 1, t − 2, and so on.\n",
      "\n",
      "What are the two subproblems derived from the context management problem by LSTMs?\n",
      "system answer:\t removing information no longer needed from the context\n",
      "real answer:\t removing information no longer needed from the context, and adding information likely to be needed for later decision making.\n",
      "\n",
      "What is the purpose of a forget gate in the LSTM architecture?\n",
      "system answer:\t delete information from the context that is no longer needed\n",
      "real answer:\t delete information from the context that is no longer needed\n",
      "\n",
      "What is expressed in the attention-based approach by comparing items in a collection?\n",
      "system answer:\t their relevance in the current context\n",
      "real answer:\t their relevance in the current context\n",
      "\n",
      "Why does the dot product output need to be scaled in the attention computation?\n",
      "system answer:\t To avoid this\n",
      "real answer:\t The result of a dot product can be an arbitrarily large (positive or negative) value. Exponentiating such large values can lead to numerical issues and to an effective loss of gradients during training.\n",
      "\n",
      "What components constitute a transformer block?\n",
      "system answer:\t inputs x and outputs y\n",
      "real answer:\t in addition to the self-attention layer, includes additional feedforward layers, residual connections, and normalizing layers\n",
      "predicted answer: from  chapter Transfer Learning with Pretrained Language Models and Contextual Embeddings, section Bidirectional Transformer Encoders, subsection nan\n",
      "actual answer:    from  chapter Deep Learning Architectures for Sequence Processing, section Self-Attention Networks: Transformers, subsection nan\n",
      "\n",
      "What is a lexical gap?\n",
      "system answer:\t unknown word problem\n",
      "real answer:\t no word or phrase, short of an explanatory footnote, can express the exact meaning of a word in the other language.\n",
      "predicted answer: from  chapter Regular Expressions, section Text Normalization, subsection Byte-Pair Encoding for Tokenization\n",
      "actual answer:    from  chapter Machine Translation and Encoder-Decoder Models, section Word Order Typology, subsection Lexical Divergences\n",
      "\n",
      "What are pro-drop languages?\n",
      "system answer:\t Languages that can omit pronouns\n",
      "real answer:\t Languages that can omit pronouns\n",
      "\n",
      "How does the beam search decoding algorithm operate?\n",
      "system answer:\t we keep k possible tokens at each step\n",
      "real answer:\t  In beam search, instead of choosing the best token beam search to generate at each timestep, we keep k possible tokens at each step.\n",
      "\n",
      "What is the difference between cross-attention and multi-head self-attention?\n",
      "system answer:\t layer norms remains the same\n",
      "real answer:\t the keys and values come from the output of the encoder.\n",
      "\n",
      "How is the chrF evaluation metric for MT computed?\n",
      "system answer:\t measuring the exact character n-grams\n",
      "real answer:\t The chrF metric is based on measuring the exact character n-grams a human reference and candidate machine translation have in common.\n",
      "\n",
      "What percentage of tokens sampled for learning are replaced with the [MASK] token during BERT pre-training?\n",
      "system answer:\t 80%\n",
      "real answer:\t 80%\n",
      "\n",
      "What is the Masked Language Modeling training objective?\n",
      "system answer:\t to predict the original inputs for each of the masked tokens\n",
      "real answer:\t predict the original inputs for each of the masked tokens\n",
      "\n",
      "What role does the [CLS] token play in BERT?\n",
      "system answer:\t embedding\n",
      "real answer:\t sentence embedding\n",
      "predicted answer: from  chapter Transfer Learning with Pretrained Language Models and Contextual Embeddings, section Training Bidirectional Encoders, subsection Masking Spans\n",
      "actual answer:    from  chapter Transfer Learning with Pretrained Language Models and Contextual Embeddings, section Transfer Learning through Fine-Tuning, subsection Sequence Classification\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dif_location = 0\n",
    "predicted_answers_list = []\n",
    "test_answers_list = []\n",
    "ans_str = 'answer'\n",
    "\n",
    "for idx, question in enumerate(questions_list):\n",
    "    answer_location = get_predicted_answer_location(answers[idx], idx)\n",
    "    pred_loc_str = get_location_str(answer_location)\n",
    "    real_loc_str = get_location_str(questions.iloc[idx])\n",
    "    location_comp = compare_book_location(answer_location, idx)\n",
    "    \n",
    "    print(question)\n",
    "    print(f\"system answer:\\t {answers[idx][ans_str]}\")\n",
    "    print(f\"real answer:\\t {questions[ans_str][idx]}\")\n",
    "    \n",
    "    predicted_answers_list.append(answers[idx]['answer'])\n",
    "    test_answers_list.append(questions['answer'][idx])\n",
    "    \n",
    "    if location_comp != \"same location\":\n",
    "        dif_location += 1\n",
    "        print(\"predicted answer: from \", pred_loc_str)\n",
    "        print(\"actual answer:    from \", real_loc_str)\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3943223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the pre-trained model to embed the answers\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Get sentence embeddings for each answer\n",
    "embeddings1 = model.encode(predicted_answers_list, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(test_answers_list, convert_to_tensor=True)\n",
    "\n",
    "# Calculate the cosine distance between the embeddings\n",
    "similarity_score = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "results = []\n",
    "\n",
    "# compute and print the scores\n",
    "for i in range(len(predicted_answers_list)):\n",
    "    result = similarity_score[i][i].item()\n",
    "    results.append(result)\n",
    "#     print(\"sent_1: \", predicted_answers_list[i])\n",
    "#     print(\"sent_2: \", test_answers_list[i])\n",
    "#     print(\"result: \", result)\n",
    "#     print()\n",
    "\n",
    "# average similarity score\n",
    "avg = sum(results) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cc8572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used for embeddings: sentence-transformers/all-MiniLM-L6-v2 \n",
      "top k: 10 \n",
      "using retrieved paragraphs from the same chapter: True\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "predicted answers from different chapters/sections/subsections than the real ones: 23\n",
      "Average similarity score between predicted and actual answers:  0.5643643845125275\n",
      "\n",
      "information retrieval elapsed time: 20.02 seconds\n",
      "total elapsed time: 53.61 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model used for embeddings: {model_name} \\ntop k: {k} \\nusing retrieved paragraphs from the same chapter: {unique_chapter}\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print()\n",
    "print(f\"predicted answers from different chapters/sections/subsections than the real ones: {dif_location}\")\n",
    "print(\"Average similarity score between predicted and actual answers: \", avg)\n",
    "print()\n",
    "print(f\"information retrieval elapsed time: {round (semantic_search_elapsed_time, 2)} seconds\")\n",
    "print(f\"total elapsed time: {round (total_elapsed_time, 2)} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
