{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ab57a8",
   "metadata": {},
   "source": [
    "# Q&A pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e18f33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: sentence-transformers in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (1.22.3)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.13.4)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (4.26.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (1.8.1)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (0.12.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.1.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U datasets pandas sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb58ef3",
   "metadata": {},
   "source": [
    "### Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aea4470",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreeaioanatudor/Desktop/Uni/Msc/NLP/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset ik-nlp-22_slp (/Users/andreeaioanatudor/.cache/huggingface/datasets/GroNLP___ik-nlp-22_slp/paragraphs/1.0.0/6c89281b2028a8a126102dda2c3fb94b1a5ccea59943d26857ae138c7aa782f8)\n",
      "100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 161.21it/s]\n",
      "Found cached dataset ik-nlp-22_slp (/Users/andreeaioanatudor/.cache/huggingface/datasets/GroNLP___ik-nlp-22_slp/questions/1.0.0/6c89281b2028a8a126102dda2c3fb94b1a5ccea59943d26857ae138c7aa782f8)\n",
      "100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 374.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['n_chapter', 'chapter', 'n_section', 'section', 'n_subsection', 'subsection', 'text'],\n",
       "        num_rows: 1697\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "paragraphs_dataset = load_dataset(\"GroNLP/ik-nlp-22_slp\", 'paragraphs')\n",
    "questions_dataset = load_dataset('GroNLP/ik-nlp-22_slp', 'questions')\n",
    "\n",
    "paragraphs_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f539e4bd",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Split the long paragraphs into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606edc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def get_chunks(paragraph, max_chunk_len=128):\n",
    "    # download the necessary resources if not already installed\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # split the text into sentences using NLTK\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "    # initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_chunk_len = 0\n",
    "\n",
    "    for s in sentences:\n",
    "        s_len = len(s.split())\n",
    "        if s_len + current_chunk_len > max_chunk_len:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = \"\"\n",
    "            current_chunk_len = 0\n",
    "        current_chunk += s\n",
    "        current_chunk_len += s_len\n",
    "        \n",
    "    if current_chunk != '':\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d34b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andreeaioanatudor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "paragraphs_long = paragraphs_dataset['train'].to_pandas()\n",
    "questions = questions_dataset['test'].to_pandas()\n",
    "\n",
    "paragraphs = pd.DataFrame(columns=paragraphs_long.columns)\n",
    "\n",
    "for index, row in paragraphs_long.iterrows():\n",
    "    if len(row['text'].split()) > max_len:\n",
    "        chunks = get_chunks(row['text'], max_len)\n",
    "    else:\n",
    "        chunks = [row['text']]\n",
    "        \n",
    "    chunk_df = pd.DataFrame(columns=row.index)\n",
    "    for chunk in chunks:\n",
    "        # Create a new dataframe for the row's chunks\n",
    "\n",
    "        new_row = row.copy()\n",
    "        new_row['text'] = chunk\n",
    "        chunk_df = pd.concat([chunk_df, new_row.to_frame().T], ignore_index=True)   \n",
    "\n",
    "    # Append the row's chunk dataframe to the new dataframe\n",
    "    paragraphs = pd.concat([paragraphs, chunk_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab91cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only paragraphs text and questions \n",
    "paragraphs_list = list(paragraphs['text'])\n",
    "questions_list = list(questions['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d860b",
   "metadata": {},
   "source": [
    "### Set the parameters for Semantic Similarity\n",
    "`model_name` - pre-trained model used for emebdding\n",
    "\n",
    "`k` - the top k paragraphs retrieved for each question (the top k highest scores)\n",
    "\n",
    "`unique_chapter` - if set to `True`, only the paragraphs coming from the same chapter will be used in the QA system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "118d9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# model_name = \"sentence-transformers/msmarco-distilbert-base-tas-b\"\n",
    "\n",
    "# k = 2\n",
    "k = 4\n",
    "\n",
    "# unique_chapter = True\n",
    "unique_chapter = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada5f23",
   "metadata": {},
   "source": [
    "### Information retrieval using Semantic Similarity\n",
    "Embed the paragraphs and questions. Then retrieve the top 10 paragraphs with the highest scores for each question (semantic similarity using cosine similarity) and store their indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d59e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Questions embeddings\n",
    "embedded_questions = model.encode(questions_list, convert_to_tensor=True)\n",
    "embedded_paragraphs = model.encode(paragraphs_list, convert_to_tensor=True)\n",
    "\n",
    "# Embed each paragraph by chunking it into smaller pieces\n",
    "# embedded_paragraphs = []\n",
    "# for paragraph in paragraphs_list:\n",
    "#     chunks = [paragraph[i:i+max_chunk_size] for i in range(0, len(paragraph), max_chunk_size)]\n",
    "#     chunk_embeddings = model.encode(chunks)\n",
    "#     paragraph_embedding = chunk_embeddings.mean(axis=0)\n",
    "#     embedded_paragraphs.append(paragraph_embedding)\n",
    "\n",
    "# embedded_paragraphs = torch.tensor(embedded_paragraphs)\n",
    "\n",
    "# top k semantic similarity score paragraphs for each question (using cosine similarity)\n",
    "sem_search_scores = util.semantic_search(query_embeddings=embedded_questions, corpus_embeddings=embedded_paragraphs, top_k=k)\n",
    "\n",
    "# stop the timer for information retrieval\n",
    "retrieval_end_time = time.time()\n",
    "\n",
    "# elapsed time for embedding + semantic search\n",
    "semantic_search_elapsed_time = retrieval_end_time - start_time\n",
    "\n",
    "contexts_idx = []\n",
    "contexts_pos = []\n",
    "\n",
    "# store the indexes of the retrieved paragraphs\n",
    "# and the start pos of each paragraph from a context \n",
    "for scores in sem_search_scores:\n",
    "    answers_idx = []\n",
    "    answers_pos = []\n",
    "    start_pos = 0\n",
    "    for answer in scores:\n",
    "        idx = answer['corpus_id']\n",
    "        answers_idx.append(idx)\n",
    "        answers_pos.append(start_pos)\n",
    "        start_pos += len(paragraphs_list[idx])\n",
    "    contexts_idx.append(answers_idx)\n",
    "    contexts_pos.append(answers_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d63f7",
   "metadata": {},
   "source": [
    "### Information Retrieval using an Instructable Model\n",
    "For each question, ask an instructable model which paragraphs are relevant to answer the question. Then, store all the relevant paragraphs (their indexes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baceddf6",
   "metadata": {},
   "source": [
    "Tried to use multithreading, but it still takes too long - can't run it for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d470763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import threading\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# import torch\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# # Define the function that each thread will execute \n",
    "# def get_relevant_context(question):\n",
    "#     answers_idx = []\n",
    "#     # Load the Flan-T5-Base model and tokenizer\n",
    "#     model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "#     tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    \n",
    "#     # find the relevant paragraphs for the question\n",
    "# #     for idx, context in enumerate(paragraphs_list):\n",
    "#     for idx, context in enumerate(paragraphs_list[0:10]):\n",
    "#         # Encode the input as a T5 sequence\n",
    "#         input_str = \"question: '{}' context: '{}' Is this context relevant for answering the question? Answer yes or no.\".format(question, context)\n",
    "#         input_ids = tokenizer.encode(input_str, return_tensors=\"pt\")\n",
    "\n",
    "#         # Generate an answer using the model\n",
    "#         outputs = model.generate(input_ids=input_ids)\n",
    "#         answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#         # if a paragraph is relevant for the question, store its index\n",
    "#         if 'yes' in answer.lower():\n",
    "#             answers_idx.append(idx)\n",
    "            \n",
    "#     return answers_idx\n",
    "\n",
    "# # Define the function that each thread will execute\n",
    "# def worker(questions, contexts_idx, start, end):\n",
    "#     for i in range(start, end):\n",
    "#         contexts_idx[i] = get_relevant_context(questions[i])\n",
    "\n",
    "# # Create a list to store the results (the indexes of the relevant paragraphs)\n",
    "# contexts_idx1 = [None] * len(questions_list)\n",
    "\n",
    "# num_threads = 15\n",
    "# chunk_size = len(questions_list) // num_threads\n",
    "\n",
    "# threads = []\n",
    "\n",
    "# # Create and start each thread\n",
    "# for i in range(num_threads):\n",
    "#     start = i * chunk_size\n",
    "#     end = start + chunk_size\n",
    "#     if i == num_threads - 1:\n",
    "#         end = len(questions_list)\n",
    "#     t = threading.Thread(target=worker, args=(questions_list, contexts_idx1, start, end))\n",
    "#     t.start()\n",
    "#     threads.append(t)\n",
    "\n",
    "# # Create a tqdm progress bar\n",
    "# with tqdm(total=len(questions_list)) as pbar:\n",
    "#     # Wait for all threads to finish\n",
    "#     for t in threads:\n",
    "#         t.join()\n",
    "#         # Update the progress bar\n",
    "#         pbar.update(chunk_size)\n",
    "\n",
    "# # The results list now contains the incremented numbers\n",
    "# print(contexts_idx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6509822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# # Load the Flan-T5-Base model and tokenizer\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# contexts_idx2 = []\n",
    "\n",
    "# for question in questions_list:\n",
    "#     answers_idx = []\n",
    "#     for idx, context in enumerate(paragraphs_list):\n",
    "#         # Encode the input as a T5 sequence\n",
    "#         input_str = \"question: '{}' context: '{}' Is this context relevant for answering the question? Answer yes or no.\".format(question, context)\n",
    "#         input_ids = tokenizer.encode(input_str, return_tensors=\"pt\")\n",
    "\n",
    "#         # Generate an answer using the model\n",
    "#         outputs = model.generate(input_ids=input_ids, max_length=32, num_beams=4)\n",
    "#         answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "#         # if a paragraph is relevant for the question, store its index\n",
    "#         if 'yes' in answer.lower():\n",
    "#             answers_idx.append(idx)\n",
    "        \n",
    "#     contexts_idx2.append(answers_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b402a",
   "metadata": {},
   "source": [
    "### Define the context for each question\n",
    "From the retrieved paragraphs of each question, keep the ones coming from the same, most common chapter, and create the context to use in the QA system. If all the top 10 (most relevant) paragraphs come from different chapters, only the first one will be used.\n",
    "\n",
    "For many sections/subsections, the value is 'nan'. So I only used chapter.\n",
    "\n",
    "`unique_chapter` must be set to `True`, otherwise all retrieved k paragraphs will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3300559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if unique_chapter:\n",
    "    book_part = 'chapter'\n",
    "    # book_part = 'n_section'\n",
    "    # book_part = 'n_subsection'\n",
    "    contexts = []\n",
    "    contexts_chapters = []\n",
    "\n",
    "    for answers_idx in contexts_idx:\n",
    "        # find the most common chapter of the k retrieved paragraphs\n",
    "        chapters = []\n",
    "        for idx in answers_idx:\n",
    "            chapters.append(paragraphs[book_part][idx])\n",
    "\n",
    "        most_common_chapter = max(chapters, key=chapters.count)\n",
    "        contexts_chapters.append(most_common_chapter)\n",
    "\n",
    "        # store the retrieved paragraphs coming from the most common chapter (and their indexes)\n",
    "        context = []\n",
    "        context_chap = []\n",
    "        for idx in answers_idx:\n",
    "            if paragraphs[book_part][idx] == most_common_chapter:\n",
    "                context.append(paragraphs['text'][idx])\n",
    "\n",
    "        contexts.append('. '.join(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "306d1225",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not unique_chapter:\n",
    "    contexts = []\n",
    "    \n",
    "    for answers_idx in contexts_idx:\n",
    "        # append the chapter of each paragraph and\n",
    "        # concatenate all paragraphs to create the context\n",
    "        chapters = []\n",
    "        context = []\n",
    "        \n",
    "        for idx in answers_idx:\n",
    "            chapters.append(paragraphs['chapter'][idx])\n",
    "            context.append(paragraphs['text'][idx])\n",
    "        contexts.append('. '.join(context))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfab89",
   "metadata": {},
   "source": [
    "### QA system\n",
    "<!-- Given the created context, answer the questions. We use roberta-base-squad2 model. -->\n",
    "Use the Inference API to access the roberta-base-squad2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4a59900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def query(payload):\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88ae02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'deepset/roberta-base-squad2'\n",
    "API_TOKEN = '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'\n",
    "API_URL = \"https://api-inference.huggingface.co/models/\" + model\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "answers = []\n",
    "\n",
    "for idx, question in enumerate(questions_list):\n",
    "    answers.append(query(\n",
    "        {\n",
    "            \"inputs\": {\n",
    "                \"question\": question,\n",
    "                \"context\": contexts[idx]\n",
    "            }\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# total elapsed time (embedding + semantic search + defining context + question answering)\n",
    "total_elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b005b27",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Print the answers of the system and the actual answers.\n",
    "Then, we compute the cosine similarity for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e371d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_answer_location(answer, q_index):\n",
    "    start_pos = answer['start']\n",
    "    end_pos = answer['end']\n",
    "    idx = None\n",
    "    \n",
    "    for i in range(len(context)-1):\n",
    "        if start_pos > contexts_pos[q_index][i] and end_pos < contexts_pos[q_index][i+1]:\n",
    "            idx = contexts_idx[q_index][i]\n",
    "            \n",
    "    if idx is None:\n",
    "        idx = contexts_idx[q_index][-1]\n",
    "    \n",
    "    return {\n",
    "        'chapter': paragraphs['chapter'][idx],\n",
    "        'section': paragraphs['section'][idx],\n",
    "        'subsection': paragraphs['subsection'][idx]\n",
    "    }\n",
    "\n",
    "\n",
    "def get_location_str(location):\n",
    "    loc_str = ''\n",
    "    loc_keys = ['chapter', 'section', 'subsection']\n",
    "    \n",
    "    for key in loc_keys:\n",
    "        loc_str += f\"{key} {location[key]}, \"\n",
    "        \n",
    "    return loc_str[:-2]\n",
    "\n",
    "\n",
    "def compare_book_location(ans_loc, q_index):\n",
    "    for key in ans_loc.keys():\n",
    "        if questions[key][q_index] != ans_loc[key]:\n",
    "            return f\"different {key}\"\n",
    "    return \"same location\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e52c44d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of the Kleene star in Regex?\n",
      "system answer:\t applies only to the space that precedes it\n",
      "real answer:\t The Kleene star means \"zero or more occurrences of the immediately previous character or regular expression\"\n",
      "predicted answer: from  chapter Regular Expressions, section Regular Expressions, subsection Disjunction, Grouping and Precendence\n",
      "actual answer:    from  chapter Regular Expressions, section Regular Expressions, subsection Basic Regular Expressions\n",
      "\n",
      "What is the usage of the Regex lookahead operator \"?=\"?\n",
      "system answer:\t pattern) is true if pattern occurs\n",
      "real answer:\t The operator (?= pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn’t advance.\n",
      "\n",
      "What are the most common steps in a text normalization process?\n",
      "system answer:\t 1. Tokenizing (segmenting) words\n",
      "real answer:\t 1. Tokenizing (segmenting) words 2. Normalizing word formats 3. Segmenting sentences\n",
      "\n",
      "What are the two most common components of a tokenization scheme?\n",
      "system answer:\t a token learner, and a token segmenter\n",
      "real answer:\t a token learner, and a token segmenter\n",
      "\n",
      "What is the purpose of a token segmenter?\n",
      "system answer:\t takes a raw test sentence and segments it into the tokens in the vocabulary\n",
      "real answer:\t The token segmenter takes a raw test sentence and segments it into the tokens in the vocabulary.\n",
      "\n",
      "What is the purpose of a token learner in the BPE algorithm?\n",
      "system answer:\t learning a vocabulary by iteratively merging tokens\n",
      "real answer:\t taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens.\n",
      "\n",
      "What is word normalization?\n",
      "system answer:\t the task of putting words/tokens in a standard format\n",
      "real answer:\t Word normalization is the task of putting words/tokens in a standard format\n",
      "\n",
      "How is lemmatization performed?\n",
      "system answer:\t determining that two words have the same root\n",
      "real answer:\t The most sophisticated methods for lemmatization involve complete morphological parsing of the word.\n",
      "predicted answer: from  chapter Regular Expressions, section nan, subsection nan\n",
      "actual answer:    from  chapter Regular Expressions, section Text Normalization, subsection Word Normalization, Lemmatization and Stemming\n",
      "\n",
      "What is lemmatization?\n",
      "system answer:\t essential for processing morphologically complex languages like Arabic\n",
      "real answer:\t Lemmatization is the task of determining that two words have the same root, despite their surface differences.\n",
      "predicted answer: from  chapter Regular Expressions, section nan, subsection nan\n",
      "actual answer:    from  chapter Regular Expressions, section Text Normalization, subsection Word Normalization, Lemmatization and Stemming\n",
      "\n",
      "How is the minimum edit distance between two strings defined?\n",
      "system answer:\t the minimum number of editing operations\n",
      "real answer:\t the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.\n",
      "\n",
      "What is a language model?\n",
      "system answer:\t n-gram models\n",
      "real answer:\t Models that assign probabilities to sequences of words are called language models\n",
      "\n",
      "How can we estimate the probability of a word?\n",
      "system answer:\t by counting the number of times every word occurs following every long string\n",
      "real answer:\t from relative frequency counts\n",
      "\n",
      "What is a Markov model?\n",
      "system answer:\t embodies the Markov assumption\n",
      "real answer:\t Markov models are the class of probabilistic models Markov that assume we can predict the probability of some future unit without looking too far into the past.\n",
      "predicted answer: from  chapter Sequence Labeling for Parts of Speech and Named Entities, section HMM Part-of-Speech Tagging, subsection Markov Chains\n",
      "actual answer:    from  chapter N-gram Language Models, section N-Grams, subsection nan\n",
      "\n",
      "What technique can be used to estimate n-gram probabilities?\n",
      "system answer:\t Smoothing algorithms\n",
      "real answer:\t maximum likelihood estimation or MLE\n",
      "predicted answer: from  chapter N-gram Language Models, section Summary, subsection nan\n",
      "actual answer:    from  chapter N-gram Language Models, section N-Grams, subsection nan\n",
      "\n",
      "What is the difference between bigram and trigram models?\n",
      "system answer:\t condition on the previous two words rather than the previous word\n",
      "real answer:\t condition on the previous two words rather than the previous word\n",
      "\n",
      "How can two probabilisting language models be compared on a test set?\n",
      "system answer:\t if they use identical vocabularies\n",
      "real answer:\t whichever model assigns a higher probability to the test set-meaning it more accurately predicts the test set-is a better model\n",
      "predicted answer: from  chapter N-gram Language Models, section Evaluating Language Models, subsection Perplexity\n",
      "actual answer:    from  chapter N-gram Language Models, section Evaluating Language Models, subsection nan\n",
      "\n",
      "What is the perplexity of a language model?\n",
      "system answer:\t perplexity 2\n",
      "real answer:\t perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words.\n",
      "predicted answer: from  chapter N-gram Language Models, section Perplexity's Relation to Entropy, subsection nan\n",
      "actual answer:    from  chapter N-gram Language Models, section Evaluating Language Models, subsection Perplexity\n",
      "\n",
      "How can language models be prevented from assigning zero probability to unknown words?\n",
      "system answer:\t choosing a fixed vocabulary in advance\n",
      "real answer:\t smoothing or discounting\n",
      "predicted answer: from  chapter N-gram Language Models, section Evaluating Language Models, subsection Perplexity\n",
      "actual answer:    from  chapter N-gram Language Models, section Smoothing, subsection nan\n",
      "\n",
      "What technique can be used to shrink an n-gram language model?\n",
      "system answer:\t pruning\n",
      "real answer:\t pruning\n",
      "\n",
      "When is a stochastic process said to be stationary?\n",
      "system answer:\t if the probabilities it assigns to a Stationary sequence are invariant\n",
      "real answer:\t A stochastic process is said to be stationary if the probabilities it assigns to a Stationary sequence are invariant with respect to shifts in the time index.\n",
      "\n",
      "Why are simplifying assumptions needed when using a Naive Bayes Classifier?\n",
      "system answer:\t huge numbers of parameters and impossibly large training sets\n",
      "real answer:\t without some simplifying assumptions, estimating the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets.\n",
      "\n",
      "What can be done to optimize Naive Bayes when an insufficient amount of labeled data is present?\n",
      "system answer:\t no optimization step\n",
      "real answer:\t derive the positive and negative word features from sentiment lexicons, lists of words that are pre-sentiment lexicons annotated with positive or negative sentiment.\n",
      "predicted answer: from  chapter Logistic Regression, section Classification: the Sigmoid, subsection Example: Sentiment Classification\n",
      "actual answer:    from  chapter Naive Bayes and Sentiment Classification, section Optimizing for Sentiment Analysis, subsection nan\n",
      "\n",
      "What type of features can be used to train a Naive Bayes classifier?\n",
      "system answer:\t dictionaries, URLs, email addresses, network features, phrases\n",
      "real answer:\t dictionaries, URLs, email addresses, network features, phrases\n",
      "\n",
      "Why is accuracy rarely used alone for unbalanced text classification tasks?\n",
      "system answer:\t accuracy doesn't work well when the classes are unbalanced\n",
      "real answer:\t because accuracy doesn't work well when the classes are unbalanced\n",
      "\n",
      "What are folds in cross-validation?\n",
      "system answer:\t k disjoint subsets\n",
      "real answer:\t k disjoints subsets\n",
      "\n",
      "What is the purpose of logistic regression?\n",
      "system answer:\t an analytic tool for testing hypotheses about the effect of various explanatory variables\n",
      "real answer:\t The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation.\n",
      "predicted answer: from  chapter Logistic Regression, section Interpreting Models, subsection nan\n",
      "actual answer:    from  chapter Logistic Regression, section Classification: the Sigmoid, subsection nan\n",
      "\n",
      "What is gradient descent?\n",
      "system answer:\t an online algorithm\n",
      "real answer:\t Gradient descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters θ ) the function's slope is rising the most steeply, and moving in the opposite direction.\n",
      "predicted answer: from  chapter Logistic Regression, section Gradient Descent, subsection The Stochastic Gradient Descent Algorithm\n",
      "actual answer:    from  chapter Logistic Regression, section Gradient Descent, subsection nan\n",
      "\n",
      "What is stochastic gradient descent?\n",
      "system answer:\t an online algorithm\n",
      "real answer:\t Stochastic gradient descent is an online algorithm that minimizes the loss function by computing its gradient after each training example\n",
      "predicted answer: from  chapter Logistic Regression, section Gradient Descent, subsection The Stochastic Gradient Descent Algorithm\n",
      "actual answer:    from  chapter Logistic Regression, section Gradient Descent, subsection nan\n",
      "\n",
      "Which regularization technique is easier to optimize?\n",
      "system answer:\t L2\n",
      "real answer:\t L2 regularization is easier to optimize because of its simple derivative\n",
      "\n",
      "What is the other name of multinomial logistic regression?\n",
      "system answer:\t binary logistic regression\n",
      "real answer:\t softmax regression\n",
      "predicted answer: from  chapter Logistic Regression, section Multinomial Logistic Regression, subsection Features in Multinomial Logistic Regression\n",
      "actual answer:    from  chapter Logistic Regression, section Multinomial Logistic Regression, subsection nan\n",
      "\n",
      "What are word connotations?\n",
      "system answer:\t affective meanings\n",
      "real answer:\t the aspects of a word's meaning that are related to a writer or reader's emotions, sentiment, opinions, or evaluations\n",
      "\n",
      "What is the idea behind vector semantics?\n",
      "system answer:\t to represent a word as a point in a multidimensional semantic space\n",
      "real answer:\t The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of word neighbors.\n",
      "\n",
      "What do rows represent in a term-document matrix?\n",
      "system answer:\t a word in the vocabulary\n",
      "real answer:\t a word in the vocabulary\n",
      "predicted answer: from  chapter Vector Semantics and Embeddings, section Words and Vectors, subsection Vectors and documents\n",
      "actual answer:    from  chapter Vector Semantics and Embeddings, section Words and Vectors, subsection nan\n",
      "\n",
      "What do columns represent in a term-document matrix?\n",
      "system answer:\t words as vectors of document counts\n",
      "real answer:\t a document from some collection of documents\n",
      "predicted answer: from  chapter Vector Semantics and Embeddings, section Words and Vectors, subsection Words as vectors: word dimensions\n",
      "actual answer:    from  chapter Vector Semantics and Embeddings, section Words and Vectors, subsection nan\n",
      "\n",
      "Why can dot product be used as a similarity metric?\n",
      "system answer:\t it will tend to be high\n",
      "real answer:\t The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions.\n",
      "\n",
      "What is the intuition behind PPMI?\n",
      "system answer:\t biased toward infrequent events\n",
      "real answer:\t PPMI draws on the intuition that the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.\n",
      "\n",
      "What is a centroid?\n",
      "system answer:\t the multidimensional version of the mean\n",
      "real answer:\t a multidimensional version of the mean\n",
      "\n",
      "What are the three type of nodes in a feedforward neural network?\n",
      "system answer:\t input units, hidden units, and output units\n",
      "real answer:\t input units, hidden units, and output units\n",
      "\n",
      "What is the output of the output layer's softmax in a neural language model?\n",
      "system answer:\t argmax\n",
      "real answer:\t a probability distribution over words\n",
      "predicted answer: from  chapter Machine Translation and Encoder-Decoder Models, section Encoder-Decoder with RNNs, subsection nan\n",
      "actual answer:    from  chapter Neural Networks and Neural Language Models, section Feedforward Neural Language Modeling, subsection Forward inference in the neural language model\n",
      "\n",
      "What is a computation graph?\n",
      "system answer:\t a representation of the process of computing a mathematical expression\n",
      "real answer:\t A computation graph is a representation of the process of computing a mathematical expression, in which the computation is broken down into separate operations, each of which is modeled as a node in a graph.\n",
      "\n",
      "What is part-of-speech tagging?\n",
      "system answer:\t process of assigning a part-of-speech to each word in a text\n",
      "real answer:\t Part-of-speech tagging is the process of assigning a part-of-speech to each word in a text.\n",
      "\n",
      "What are named entities?\n",
      "system answer:\t anything that can be referred to with a proper name\n",
      "real answer:\t A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization.\n",
      "\n",
      "What is a Hidden Markov Model?\n",
      "system answer:\t like words that we see in the input\n",
      "real answer:\t An HMM is a probabilistic sequence model\n",
      "predicted answer: from  chapter Sequence Labeling for Parts of Speech and Named Entities, section HMM Part-of-Speech Tagging, subsection The Hidden Markov Model\n",
      "actual answer:    from  chapter Sequence Labeling for Parts of Speech and Named Entities, section HMM Part-of-Speech Tagging, subsection nan\n",
      "\n",
      "Why is limited context a limitation of feedforward neural networks?\n",
      "system answer:\t Anything outside the context window has no impact on the decision being made\n",
      "real answer:\t Anything outside the context window has no impact on the decision being made.\n",
      "\n",
      "What is a recurrent neural network?\n",
      "system answer:\t any network that contains a cycle within its network connections\n",
      "real answer:\t any network that contains a cycle within its network connections\n",
      "\n",
      "What is an autoregressive language model?\n",
      "system answer:\t a model that predicts a value at time t\n",
      "real answer:\t a model that predicts a value at time t based on a linear function of the previous values at times t − 1, t − 2, and so on.\n",
      "\n",
      "What are the two subproblems derived from the context management problem by LSTMs?\n",
      "system answer:\t removing information no longer needed from the context\n",
      "real answer:\t removing information no longer needed from the context, and adding information likely to be needed for later decision making.\n",
      "\n",
      "What is the purpose of a forget gate in the LSTM architecture?\n",
      "system answer:\t delete information from the context that is no longer needed\n",
      "real answer:\t delete information from the context that is no longer needed\n",
      "\n",
      "What is expressed in the attention-based approach by comparing items in a collection?\n",
      "system answer:\t their relevance in the current context\n",
      "real answer:\t their relevance in the current context\n",
      "\n",
      "Why does the dot product output need to be scaled in the attention computation?\n",
      "system answer:\t makes it extremely expensive for the input to a transformer to consist of long documents\n",
      "real answer:\t The result of a dot product can be an arbitrarily large (positive or negative) value. Exponentiating such large values can lead to numerical issues and to an effective loss of gradients during training.\n",
      "\n",
      "What components constitute a transformer block?\n",
      "system answer:\t residual connections are used with both the attention and feedforward sublayers\n",
      "real answer:\t in addition to the self-attention layer, includes additional feedforward layers, residual connections, and normalizing layers\n",
      "predicted answer: from  chapter Deep Learning Architectures for Sequence Processing, section Self-Attention Networks: Transformers, subsection Transformer Blocks\n",
      "actual answer:    from  chapter Deep Learning Architectures for Sequence Processing, section Self-Attention Networks: Transformers, subsection nan\n",
      "\n",
      "What is a lexical gap?\n",
      "system answer:\t no word or phrase, short of an explanatory footnote\n",
      "real answer:\t no word or phrase, short of an explanatory footnote, can express the exact meaning of a word in the other language.\n",
      "\n",
      "What are pro-drop languages?\n",
      "system answer:\t Languages that can omit pronouns\n",
      "real answer:\t Languages that can omit pronouns\n",
      "\n",
      "How does the beam search decoding algorithm operate?\n",
      "system answer:\t incrementally adding the logprob of generating each next token\n",
      "real answer:\t  In beam search, instead of choosing the best token beam search to generate at each timestep, we keep k possible tokens at each step.\n",
      "\n",
      "What is the difference between cross-attention and multi-head self-attention?\n",
      "system answer:\t the keys and values come from the output of the encoder\n",
      "real answer:\t the keys and values come from the output of the encoder.\n",
      "\n",
      "How is the chrF evaluation metric for MT computed?\n",
      "system answer:\t measuring the exact character n-grams\n",
      "real answer:\t The chrF metric is based on measuring the exact character n-grams a human reference and candidate machine translation have in common.\n",
      "\n",
      "What percentage of tokens sampled for learning are replaced with the [MASK] token during BERT pre-training?\n",
      "system answer:\t 80%\n",
      "real answer:\t 80%\n",
      "\n",
      "What is the Masked Language Modeling training objective?\n",
      "system answer:\t to predict the original inputs for each of the masked tokens\n",
      "real answer:\t predict the original inputs for each of the masked tokens\n",
      "\n",
      "What role does the [CLS] token play in BERT?\n",
      "system answer:\t compute recall\n",
      "real answer:\t sentence embedding\n",
      "predicted answer: from  chapter Machine Translation and Encoder-Decoder Models, section MT Evaluation, subsection Automatic Evaluation: Embedding-Based Methods\n",
      "actual answer:    from  chapter Transfer Learning with Pretrained Language Models and Contextual Embeddings, section Transfer Learning through Fine-Tuning, subsection Sequence Classification\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dif_location = 0\n",
    "predicted_answers_list = []\n",
    "test_answers_list = []\n",
    "ans_str = 'answer'\n",
    "\n",
    "for idx, question in enumerate(questions_list):\n",
    "    answer_location = get_predicted_answer_location(answers[idx], idx)\n",
    "    pred_loc_str = get_location_str(answer_location)\n",
    "    real_loc_str = get_location_str(questions.iloc[idx])\n",
    "    location_comp = compare_book_location(answer_location, idx)\n",
    "    \n",
    "    print(question)\n",
    "    print(f\"system answer:\\t {answers[idx][ans_str]}\")\n",
    "    print(f\"real answer:\\t {questions[ans_str][idx]}\")\n",
    "    \n",
    "    predicted_answers_list.append(answers[idx]['answer'])\n",
    "    test_answers_list.append(questions['answer'][idx])\n",
    "    \n",
    "    if location_comp != \"same location\":\n",
    "        dif_location += 1\n",
    "        print(\"predicted answer: from \", pred_loc_str)\n",
    "        print(\"actual answer:    from \", real_loc_str)\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3943223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the pre-trained model to embed the answers\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Get sentence embeddings for each answer\n",
    "embeddings1 = model.encode(predicted_answers_list, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(test_answers_list, convert_to_tensor=True)\n",
    "\n",
    "# Calculate the cosine distance between the embeddings\n",
    "similarity_score = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "results = []\n",
    "\n",
    "# compute and print the scores\n",
    "for i in range(len(predicted_answers_list)):\n",
    "    result = similarity_score[i][i].item()\n",
    "    results.append(result)\n",
    "#     print(\"sent_1: \", predicted_answers_list[i])\n",
    "#     print(\"sent_2: \", test_answers_list[i])\n",
    "#     print(\"result: \", result)\n",
    "#     print()\n",
    "\n",
    "# average similarity score\n",
    "avg = sum(results) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cc8572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used for embeddings: sentence-transformers/all-MiniLM-L6-v2 \n",
      "top k: 4 \n",
      "using retrieved paragraphs from the same chapter: False\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "predicted answers from different chapters/sections/subsections than the real ones: 19\n",
      "Average similarity score between predicted and actual answers:  0.6064071760718095\n",
      "\n",
      "information retrieval elapsed time: 19.67 seconds\n",
      "total elapsed time: 57.1 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model used for embeddings: {model_name} \\ntop k: {k} \\nusing retrieved paragraphs from the same chapter: {unique_chapter}\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print()\n",
    "print(f\"predicted answers from different chapters/sections/subsections than the real ones: {dif_location}\")\n",
    "print(\"Average similarity score between predicted and actual answers: \", avg)\n",
    "print()\n",
    "print(f\"information retrieval elapsed time: {round (semantic_search_elapsed_time, 2)} seconds\")\n",
    "print(f\"total elapsed time: {round (total_elapsed_time, 2)} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
